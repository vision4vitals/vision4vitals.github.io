<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

        <title>V4V 2021 - Vision for Vitals (ICCV 2021)</title>
        <!-- Bootstrap Core CSS -->
        <link href="./bootstrap.min.css" rel="stylesheet">
        <link href="./bootstrap-social.css" rel="stylesheet">
        <link href="./custom.css" rel="stylesheet">
        <!-- Custom CSS -->
        <link href="./modern-business.css" rel="stylesheet">
        <!-- Custom Fonts -->
        <link href="./font-awesome.min.css" rel="stylesheet" type="text/css">
		
		<script src="./jquery.js.download"></script>
		<script src="./bootstrap.min.js.download"></script>

		<style>
			body {
				background-color: #24343d;
				color: #d7dee3;
			}
		</style>		
		
    </head>
    <body background="bg_dark3.png">
        <!-- Page Content -->
        <div class="container">
            <!-- Marketing Icons Section -->
            <div class="row" align="center">
				<img src="logo4.png" class="img-responsive" align="center">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
                <h1 class="page-header first-header text-center" style="border-bottom:none"> V4V Challenge </h1>
				<h2 class="page-header first-header text-center" style="border-bottom:none">Vision for Vitals</h2>
                <h4 class="text-center">In conjunction with <a href="http://iccv2021.thecvf.com/home">ICCV 2021, Montreal, Canada</a><br>
				</h4>
                <br>
				</div>				
            </div>
            
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Keynote Speakers</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1 align-self-center justify-content-center ">
                    <!-- <div class="col-lg-4 col-md-4"> -->
                    
                <p class="h4">
                    #1. Keynote title TBA
                </p>
                <div class="col-lg-3 col-md-3 col-sm-3 text-center align-middle">
                    <div
                            class="border d-flex align-items-center justify-content-center"
                            style="height: 350px;">
                    <img style="padding-top: 25px;" src="conradt.png" class="img-responsive img-rounded center-block">    
                    <p class="h4"><a href="#">Conrad S. Tucker</a></p>
                </div>
            </div>
                <div class="col-lg-9 col-md-9 col-sm-9">
                    <p class="text-justify">
                    Dr. Conrad Tucker is an Arthur Hamerschlag Career Development Professor of Mechanical Engineering and holds courtesy appointments in Machine Learning, Robotics, Biomedical Engineering and CyLab Security & Privacy Institute
                    at Carnegie Mellon University. His research focuses on the design and optimization of systems through the acquisition, integration and mining of large scale, disparate data.
                    </p>
                    <p class="text-justify">
                    Dr. Tucker has served as PI/Co-PI on federally/non-federally funded grants from the National Science Foundation (NSF), the Air Force Office of Scientific Research (AFOSR), the Defense Advanced Research Projects Agency (DARPA), the Army Research Laboratory (ARL), the Office of Naval Research (ONR) via the NSF Center for eDesign, and the Bill and Melinda Gates Foundation (BMGF). In February 2016, he was invited by National Academy of Engineering (NAE) President Dr. Dan Mote, to serve as a member of the Advisory Committee for the NAE Frontiers of Engineering Education (FOEE) Symposium. He received his Ph.D., M.S. (Industrial Engineering), and MBA degrees from the University of Illinois at Urbana-Champaign, and his B.S. in Mechanical Engineering from Rose-Hulman Institute of Technology.
                    </p>
                    
                </div>

                </div>
                
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1 padding:20px">
                
                    <p class="h4">
                       #2. Seeing Inside Out: Camera-based Physiological Sensing with Applications in Telehealth and Wellbeing
                       
                    </p>

                    <div class="col-lg-3 col-md-3 col-sm-3 text-center">
                        <img style="padding-top: 40px; " src="Daniel_McDuff_Headshot3-2.jpg" class="img-responsive img-rounded center-block">    
                        <p class="h4"><a href="#">Daniel McDuff</a></p>
                    </div>
                    <div class="col-lg-9 col-md-9 col-sm-9">
                        <p class="text-justify">
                            <p class="text-justify">
                            Daniel McDuff is a Principal Researcher at Microsoft.  Daniel completed his PhD at the MIT Media Lab in 2014 and has a B.A. and Masters from Cambridge University. Daniel's work on non-contact physiological measurement helped to popularize a new field of low-cost health monitoring using webcams. Previously, Daniel worked at the UK MoD, was Director of Research at MIT Media Lab spin-out Affectiva and a post-doctoral research affiliate at MIT.  His work has received nominations and awards from Popular Science magazine as one of the top inventions in 2011, South-by-South-West Interactive (SXSWi), The Webby Awards, ESOMAR and the Center for Integrated Medicine and Innovative Technology (CIMIT). His projects have been reported in many publications including The Times, the New York Times, The Wall Street Journal, BBC News, New Scientist, Scientific American and Forbes magazine. Daniel was named a 2015 WIRED Innovation Fellow, an ACM Future of Computing Academy member and has spoken at TEDx and SXSW.  Daniel has published over 100 peer-reviewer papers on machine learning (NeurIPS, ICLR, ICCV, ECCV, ACM TOG), human-computer interaction (CHI, CSCW, IUI) and biomedical engineering (TBME, EMBC). 
                            </p>
                        </p>
                        
                    </div>
                
                </div>



            </div>

            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Description</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <p class="text-justify">
                        Telehealth has the potential to offset the high demand for help during public health emergencies,
                        such as the ongoing COVID pandemic, and in rural locations where health services and qualified
                        treatment providers make services difficult if not impossible to obtain. Besides communication,
                        the use of existing sensor infrastructure within modern smart devices for medical tests are
                        compelling. Remote Photoplethysmography (rPPG) - the problem of non-invasively estimating
                        blood volume variations in the microvascular tissue from video - would be well suited for these
                        situations.
                    </p>
                    <p class="text-justify">

                        Over the past few years a number of research groups have made rapid advances in remote PPG
                        methods for estimating heart rate from digital video and obtained impressive results. How these
                        various methods compare in naturalistic conditions, where spontaneous movements, facial
                        expressions, or illumination changes are present, is relatively unknown. Most previous
                        benchmarking efforts focused on posed situations. No commonly accepted evaluation protocol
                        exists for estimating vital signs in spontaneous behavior with which to compare them.


                    </p>
                    <p class="text-justify">
                        To enable comparisons among alternative methods, we present the 1st Vision for Vitals Workshop
                        & Challenge (V4V 2021). This topic is germane to both computer vision and multimedia
                        communities. For computer vision, it is an exciting approach to longstanding limitations of vital
                        signs estimating approaches. For multimedia, remote vital signs estimation would enable more
                        powerful applications.
                    </p>

                    
                </div>
            </div>
			
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Main track</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <p class="text-justify">
                        The main track is intended to bring together computer vision researchers whose work is
                        related to vision based vital signs estimation. We are soliciting original contributions which
                        address a wide range of theoretical and application issues of remote vital signs estimation,
                        including but not limited to:

                        </p>
                        <li>Methods for extracting vital signals from videos, including pulse rate, respiration rate,
                          blood oxygen, and body temperature. </li>

                          <li> Vision-based methods to support and augment vital signs monitoring systems, such as 
                          face/skin detection, motion tracking, video segmentation, and optimization. </li>
                          <li> Vision-based vital signs measurement for affective, emotional, or cognitive states. </li>
                          <li> Vision-based vital signs measurement to assist video surveillance in-the-wild. </li>
                          <li> Vision-based vital signs measurement to detect human liveness or manipulated images
                          (deep fake detection).  </li>
                          <li> Applications of vision-based vital signs monitoring </li>
                          <li> User interfaces employing vision-based vital signs estimation </li>
                    <p></p>
                </div>
            </div>
			
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Challenge track</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <p class="text-justify">
                        V4V Challenge evaluates remote PPG methods for vital signs estimation on a new large
                        corpora of face videos annotated with corresponding high-resolution videos and vital signs from contact sensors. The goal of the challenge is
                        to reconstruct the vital signs of the subjects from the video sources. The participants will
                        receive an annotated training set and a test set without annotations. 
                    </p>
                    

                    <p> There are two subtracks in the challenge - (1) Heart rate (HR) estimation and (2) Respiration rate (RR) estimation. All participants are required to submit predictions to HR sub-challenge. Although, participation in RR sub-challenge is optional, we encourage all participants to involve in both sub-challenges. To learn more, please head over to this <a href="https://competitions.codalab.org/competitions/31978#learn_the_details-submission">page</a> on Codalab.</p>

                    <p>
                        The datasets may be used for the V4V Challenge of ICCV 2021 only. The recipient of the datasets must be a full-time faculty, researcher or employee of an organization (not a student) and <b>must agree to <a href="https://competitions.codalab.org/competitions/31978#learn_the_details-terms_and_conditions">terms and conditions</a></b> listed on the codalab.
                    </p>


                    <p class="lead">

            If you are interested in downloading the V4V dataset please <a href="./V4V_EULA-ICCV2021.pdf">download and sign the EULA</a> and <a href="mailto:lijun@cs.binghamton.edu,laszlojeni@cmu.edu,zli191@binghamton.edu?subject=V4V Challenge data request&amp">email the scanned copy back to lijun(at)cs(dot)binghamton(dot)edu, zli191(at)binghamton(dot)edu and laszlojeni(at)cmu(dot)edu</a >
                    </p>

                </div>
            </div>


            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Evaluation and Submissions</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <p class="text-justify">
                        Please visit the <a href="https://competitions.codalab.org/competitions/31978">Codalab page</a> where the competition is hosted. Additionally, the evaluation code can be downloaded <a href="https://github.com/vision4vitals/v4v_eval_scripts/blob/main/evaluate_Codalab.py">here</a> for local use by participants. The requirements file for the local environment setup can be downloaded from the same repository. Please report any bugs in the evaluation code in the issues of the repository.
			</p> 
			<p class="text-justify">
			Please note that along with your submission to Codalab competition page, in <b>order to be included in the leaderboard, you are required to submit a short paper containing the description of your method</b>. For paper submission to the workshop, visit our <a href="https://cmt3.research.microsoft.com/V4V2021/">CMT page</a>. When submitting your paper to CMT, please also email <a href="mailto:arevanur@andrew.cmu.edu?subject=V4V Challenge Codalab username/paper title&amp">arevanur(at)andrew(dot)cmu(dot)edu</a> with your username/Team name on Codalab, and your workshop paper title, to make it easier to link your paper to the Codalab submissions. 
			</p>
			<p class="text-justify">
			Challenge paper submissions must be written in English and must be sent in PDF format. Please refer to the ICCV submission guidelines for instructions regarding formatting, templates, and policies. The submissions will be reviewed by the program committee and selected papers will be published in ICCV Workshop proceedings.
		    </p> 

                </div>
            </div>
            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Dates</h2>
					<p><b>Challenge Track</b>
						<ul>
                            <li>May 21th: Challenge site opens, training data available</li>
                            <li>July 9th: Testing phase begins</li>
                            <li>July 30th: Competition ends (challenge paper submission - optional)</li>
                        </ul>
						</p>
					<p><b>Workshop Track</b>
						<ul>
                            <li>July 26th: Paper submission deadline</li>
                            <li>August 9th: Notification of acceptance</li>
                            <li>August 16th: Camera ready submission</li>
                        </ul>
						</p>
                </div>
            </div>


            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Workshop chairs</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <ul>
                        <li><a href="http://www.laszlojeni.com/">Laszlo A. Jeni</a>, Carnegie Mellon University, USA</li>
                        <li><a href="http://www.cs.binghamton.edu/~lijun/">Lijun Yin</a>, Binghamton University, USA</li>
                    </ul>
              
			<p><b>Data chairs</b>
			<ul>
                        <li><a href="https://www.linkedin.com/in/ambareeshr/">Ambareesh Revanur</a>, Carnegie Mellon University, USA</li>
                        <li><a href="https://www.linkedin.com/in/zhihua-li/">Zhihua Li</a>, Binghamton University, USA</li>
			            <li><a href="">Umur A. Ciftci</a>, Binghamton University, USA</li>
                        </ul>
			</p>
           
            </div>
        </div>

            <div class="row">
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
                    <h2 class="page-header">Technical program committee</h2>
                </div>
                <div class="col-lg-10 col-md-10 col-sm-12 col-sm-offset-0 col-xs-10 col-xs-offset-1 col-md-offset-1 col-lg-offset-1">
			<ul>
                <li>Sergio Escalera, Universitat of Barcelona</li>
                <li>Shaun Canavan, University of South Florida</li>
                <li>Vitomir Struc, University of Ljubljana</li>
                <li>Itir Onal Ertugrul, Tilburg University</li>
                <li>Michel Valstar, University of Nottingham</li>
                <li>Abhinav Dhall, Monash University</li>
                <li>Saurabh Hinduja, University of Pittsburgh</li>
                <li>Tim K Marks, Mitsubishi Electric Research Labs (MERL)</li>
			</ul>
                </div>
            </div>
		    </div>
		
    
	

	

</div></body></html>
